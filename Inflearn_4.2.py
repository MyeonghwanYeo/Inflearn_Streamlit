"""
LangChainìœ¼ë¡œ ì‘ì„±í•œ ì½”ë“œë¥¼ í™œìš©í•œ LLM ë‹µë³€
1. RAGê°€ ì ìš©ëœ ê¸°ì¡´ LLM ì½”ë“œ ë¶ˆëŸ¬ì˜¤ê¸°
2. get_ai_message í•¨ìˆ˜ ì •ì˜ë¥¼ í†µí•´ AI ë©”ì„¸ì§€ ìƒì„±
"""

import streamlit as st

from dotenv import load_dotenv
from langchain_upstage import UpstageEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain_upstage import ChatUpstage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langsmith import Client

load_dotenv()

st.set_page_config(page_title="ì†Œë“ì„¸ ì±—ë´‡", page_icon="ğŸ¤–")

# Streamlitì—ì„œ Titleì„ "h1"ìœ¼ë¡œ ìë™ ì„¤ì •
st.title("ğŸ¤– ì†Œë“ì„¸ ì±—ë´‡")
st.caption("ì†Œë“ì„¸ì™€ ê´€ë ¨ëœ ëª¨ë“  ê²ƒì„ ë‹µí•´ë“œë¦½ë‹ˆë‹¤!")

def get_ai_message(user_input):
    # ì„ë² ë”© ëª¨ë¸ ì €ì¥
    embedding = UpstageEmbeddings(
        api_key="up_tknqyPYEHnMeHX0wnaofSJhwhYWRf",
        model='embedding-query'
    )

    database = PineconeVectorStore.from_existing_index(
        index_name='tax-markdown-index',
        embedding=embedding,
    )

    llm = ChatUpstage(
        api_key="up_tknqyPYEHnMeHX0wnaofSJhwhYWRf",
        model='solar-mini'
    )

    dictionary = ["ì‚¬ëŒì„ ë‚˜íƒ€ë‚´ëŠ” í‘œí˜„ -> ê±°ì£¼ì"]

    # ëª¨ë¸ì´ ë¬¸ì„œì˜ ì •ë³´ë¥¼ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì§ˆë¬¸ ë³€ê²½ Prompt ìƒì„±
    prompt_dict = ChatPromptTemplate.from_template(f"""
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³´ê³ , ìš°ë¦¬ì˜ ì‚¬ì „ì„ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³€ê²½í•´ì£¼ì„¸ìš”.
    ë§Œì•½ ë³€ê²½í•  í•„ìš”ê°€ ì—†ë‹¤ê³  íŒë‹¨ëœë‹¤ë©´, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³€ê²½í•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤.
    ê·¸ëŸ° ê²½ìš°ì—ëŠ” ì§ˆë¬¸ë§Œ ë¦¬í„´í•´ì£¼ì„¸ìš”.
    ì‚¬ì „: {dictionary}

    ì§ˆë¬¸: {{question}}
    """)

    dictionary_chain = prompt_dict | llm | StrOutputParser()

    client = Client()
    prompt = client.pull_prompt("rlm/rag-prompt")

    chain = (
            {
                "context": lambda x: database.similarity_search(
                    dictionary_chain.invoke({"question": x["question"]}),
                    k=3),
                "question": dictionary_chain,
            }
            | prompt
            | llm
    )

    ai_message = chain.invoke({"question": user_input}).content

    return ai_message


# message_list: ì…ë ¥ëœ ì±„íŒ… ë‚´ìš© ì €ì¥
if 'message_list' not in st.session_state:
    st.session_state.message_list = []

for message in st.session_state.message_list:
    with st.chat_message(message["role"]):
        st.write(message["content"])

if user_question := st.chat_input(placeholder="ì†Œë“ì„¸ì™€ ê´€ë ¨ëœ ê¶ê¸ˆí•œ ë‚´ìš©ë“¤ì„ ë§ì”€í•´ì£¼ì„¸ìš”."):
    with st.chat_message("user"):
        st.write(user_question)
    st.session_state.message_list.append({"role": "user", "content": user_question})

    # ë¡œë”© í‘œì‹œ ìƒì„±
    with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤."):
        ai_message = get_ai_message(user_input=user_question)

        with st.chat_message("ai"):
            st.write(ai_message)
        st.session_state.message_list.append({"role": "ai", "content": ai_message})